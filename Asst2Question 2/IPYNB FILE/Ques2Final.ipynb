{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 112: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-17dc9fedcda6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\smrut\\Documents\\Python Assignments\\Data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mjson_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[1;31m# here you need to know the layout of your json and each json has to have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\smrut\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \"\"\"\n\u001b[0;32m--> 296\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\smrut\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 112: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "# defininf dictionaries for country and state\n",
    "dict_country={}\n",
    "term_dict={}\n",
    "country_list=[]\n",
    "state_dir={}\n",
    "# this finds our json files\n",
    "path_to_json = 'json/'\n",
    "json_files = [pos_json for pos_json in os.listdir(r'C:\\Users\\smrut\\Documents\\Data Analysis using Python\\DataAnalysis4Python_Spring17\\Data\\DataAnalysis4Python_Spring17\\Assignment 2\\Data') if pos_json.endswith('.json')]\n",
    "\n",
    "# The dataframe with the required columns is defined\n",
    "jsons_data = pd.DataFrame(columns=['country', 'state', 'city'])\n",
    "\n",
    "# using enumerate to get the index and the json file\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(r'C:\\Users\\smrut\\Documents\\Python Assignments\\Data', js)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        # extracting the terms 'term',restaurant', 'location' from the json files\n",
    "        \n",
    "        term= json_text['term']\n",
    "        country = json_text['location']['country']\n",
    "        state = json_text['location']['state']\n",
    "        city = json_text['location']['city']\n",
    "        # this is the path to which the fodlers are copied based on the hierarchy we want\n",
    "        newpath = ((r'Data_Processed\\term_%s\\country_%s\\state_%s\\city_%s') % (term,country,state,city))\n",
    "        if not os.path.exists(newpath): os.makedirs(newpath)\n",
    "        \n",
    "        \n",
    "    shutil.move(r'C:\\Users\\smrut\\Documents\\Python Assignments\\Data\\%s' % js, newpath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "csvfile=open(r'restaurant.csv', 'w',encoding='utf-8',newline=\"\")#csv file is opened\n",
    "writer=csv.writer(csvfile)  # csv file is written using csv.writer\n",
    "writer.writerow(['Name of Restaurant','City','Country Code','Day of Week','Start Time Hour','Start Time Minute','End Time Hour','End Time Minute']) #writer to write the row\n",
    "path_to_json = r'C:\\Users\\smrut\\Documents\\Data Analysis using Python\\DataAnalysis4Python_Spring17\\Data\\DataAnalysis4Python_Spring17\\Assignment 2\\Data'\n",
    "for js in json_files: # looping through the json files\n",
    "    with open(os.path.join(path_to_json, js)) as json_file: #the path of json files are appended\n",
    "        loaded_file = json.load(json_file) #load the json file\n",
    "        for key,value in loaded_file.items():\n",
    "             if value=='restaurants':  # the 'term' for restaurants is searched in the json files\n",
    "                    name=loaded_file['name'] # name of the column is set\n",
    "                    for key,value in loaded_file.items():\n",
    "                        if key=='location':\n",
    "                            city=value.get('city') #the values for city and country in the rows are got\n",
    "                            country=value.get('country')\n",
    "                            for key,value in loaded_file.items():       \n",
    "                                if key=='hours':\n",
    "                                    for v in value:\n",
    "                                        for k,v in v.items():\n",
    "                                            if k=='open':\n",
    "                                                for a in v:\n",
    "                                                    st= datetime.datetime.strptime(a['start'],\"%H%M\") #use strip time function to strip the hours and minute from the start time\n",
    "                                                    end=datetime.datetime.strptime(a['end'],\"%H%M\") # use strip time function to strip  the hours and minute from the end time                       \n",
    "                                                    writer.writerow([name,city,country,a['day'],st.hour,st.minute,end.hour,end.minute]) #export the data in the required csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
